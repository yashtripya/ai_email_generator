{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBnvw/mCJ5XWV1qSXmvV2N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yashtripya/ai_email_generator/blob/main/Gen_AI_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "k8ml94ee5ChY",
        "outputId": "fb3b83fa-00f9-40aa-c209-d5e805af7ca0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 🧹 Clean notebook metadata to avoid GitHub rendering errors (ONLY when running in Colab)\n",
        "try:\n",
        "    import google.colab\n",
        "    import nbformat\n",
        "    from IPython import get_ipython\n",
        "\n",
        "    ipython = get_ipython()\n",
        "    if ipython:\n",
        "        notebook_path = ipython.run_line_magic('pwd', '') + '/temp_notebook.ipynb'\n",
        "        ipython.run_line_magic('save', 'temp_notebook.ipynb 0-999')\n",
        "\n",
        "        nb = nbformat.read(open(notebook_path, encoding=\"utf-8\"), as_version=nbformat.NO_CONVERT)\n",
        "        if 'widgets' in nb['metadata']:\n",
        "            del nb['metadata']['widgets']\n",
        "            nbformat.write(nb, notebook_path)\n",
        "            print(\"✅ Cleaned notebook metadata to fix GitHub rendering issue.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"⚠️ Skipping metadata cleaning (not running in Colab or error occurred).\")\n",
        "    print(e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0NXZfQU2fau",
        "outputId": "db922ec6-d24e-48be-fbf9-3e252132d592"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following commands were written to file `temp_notebook.ipynb.py`:\n",
            "\n",
            "get_ipython().system('pip install transformers accelerate')\n",
            "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
            "import torch\n",
            "\n",
            "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
            "\n",
            "print(\"🔄 Loading the model. Please wait...\")\n",
            "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
            "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
            "\n",
            "\n",
            "generator = pipeline(\n",
            "    \"text-generation\",\n",
            "    model=model,\n",
            "    tokenizer=tokenizer,\n",
            "    torch_dtype=torch.float32,\n",
            "    device=0 if torch.cuda.is_available() else -1\n",
            ")\n",
            "print(\"✅ Model loaded successfully!\")\n",
            "\n",
            "def build_prompt(purpose, recipient, points, tone, sender):\n",
            "    bullet_text = '\\n'.join(f\"- {point}\" for point in points)\n",
            "    return (\n",
            "        f\"Write a {tone} email to a {recipient} regarding: {purpose}.\\n\"\n",
            "        f\"Include the following key points:\\n{bullet_text}\\n\"\n",
            "        f\"Start with a subject line, then write the body.\\n\"\n",
            "        f\"End the email with: \\nRegards,\\n{sender}\\n\\n\"\n",
            "        f\"Subject:\"\n",
            "    )\n",
            "\n",
            "def generate_text(prompt):\n",
            "    output = generator(\n",
            "        prompt,\n",
            "        max_new_tokens=250,\n",
            "        do_sample=True,\n",
            "        temperature=0.7,\n",
            "        top_k=50,\n",
            "        top_p=0.95\n",
            "    )\n",
            "    raw_text = output[0][\"generated_text\"].strip()\n",
            "    return clean_output(raw_text, prompt)\n",
            "\n",
            "def clean_output(text, prompt):\n",
            "    if text.startswith(prompt):\n",
            "        text = text[len(prompt):].strip()\n",
            "    last_punc = max(text.rfind(p) for p in [\".\", \"!\", \"?\"])\n",
            "    return text[:last_punc + 1].strip() if last_punc != -1 else text.strip()\n",
            "\n",
            "def get_inputs():\n",
            "    print(\"\\n\" + \"=\" * 60)\n",
            "    print(\"📬 AI EMAIL GENERATOR\".center(60))\n",
            "    print(\"=\" * 60)\n",
            "\n",
            "    sender = input(\"✍️  Enter your name (Sender): \").strip()\n",
            "    recipient = input(\"👤 Who is the recipient? (e.g., Manager, Friend): \").strip()\n",
            "    purpose = input(\"📝 What is the purpose of the email? (e.g., job application, invite): \").strip()\n",
            "\n",
            "    print(\"\\n🧾 Enter bullet points you want included (type 'done' to finish):\")\n",
            "    points = []\n",
            "    while True:\n",
            "        item = input(\"➤ \").strip()\n",
            "        if item.lower() == \"done\":\n",
            "            break\n",
            "        if item:\n",
            "            points.append(item)\n",
            "\n",
            "    print(\"\\n🎯 Choose the tone of your email:\\n1. Formal\\n2. Informal\")\n",
            "    tone = \"formal\" if input(\"Select option (1 or 2): \").strip() == \"1\" else \"informal\"\n",
            "\n",
            "    return purpose, recipient, points, tone, sender\n",
            "\n",
            "def main():\n",
            "    purpose, recipient, points, tone, sender = get_inputs()\n",
            "    prompt = build_prompt(purpose, recipient, points, tone, sender)\n",
            "    print(\"\\n✨ Generating your email. Please wait...\")\n",
            "    result = generate_text(prompt)\n",
            "    print(\"\\n\" + \"=\" * 60)\n",
            "    print(\"📨 Generated Email:\\n\")\n",
            "    print(result)\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "# 🧹 Clean notebook metadata to avoid GitHub rendering errors (ONLY when running in Colab)\n",
            "try:\n",
            "    import google.colab\n",
            "    import nbformat\n",
            "    from IPython import get_ipython\n",
            "\n",
            "    ipython = get_ipython()\n",
            "    if ipython:\n",
            "        notebook_path = ipython.run_line_magic('pwd', '') + '/temp_notebook.ipynb'\n",
            "        ipython.run_line_magic('save', 'temp_notebook.ipynb 0-999')\n",
            "\n",
            "        nb = nbformat.read(open(notebook_path, encoding=\"utf-8\"), as_version=nbformat.NO_CONVERT)\n",
            "        if 'widgets' in nb['metadata']:\n",
            "            del nb['metadata']['widgets']\n",
            "            nbformat.write(nb, notebook_path)\n",
            "            print(\"✅ Cleaned notebook metadata to fix GitHub rendering issue.\")\n",
            "\n",
            "except Exception as e:\n",
            "    print(\"⚠️ Skipping metadata cleaning (not running in Colab or error occurred).\")\n",
            "    print(e)\n",
            "⚠️ Skipping metadata cleaning (not running in Colab or error occurred).\n",
            "[Errno 2] No such file or directory: '/content/temp_notebook.ipynb'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch\n",
        "\n",
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "print(\"🔄 Loading the model. Please wait...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.float32,\n",
        "    device=0 if torch.cuda.is_available() else -1\n",
        ")\n",
        "print(\"✅ Model loaded successfully!\")\n",
        "\n",
        "def build_prompt(purpose, recipient, points, tone, sender):\n",
        "    bullet_text = '\\n'.join(f\"- {point}\" for point in points)\n",
        "    return (\n",
        "        f\"Write a {tone} email to a {recipient} regarding: {purpose}.\\n\"\n",
        "        f\"Include the following key points:\\n{bullet_text}\\n\"\n",
        "        f\"Start with a subject line, then write the body.\\n\"\n",
        "        f\"End the email with: \\nRegards,\\n{sender}\\n\\n\"\n",
        "        f\"Subject:\"\n",
        "    )\n",
        "\n",
        "def generate_text(prompt):\n",
        "    output = generator(\n",
        "        prompt,\n",
        "        max_new_tokens=250,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.95\n",
        "    )\n",
        "    raw_text = output[0][\"generated_text\"].strip()\n",
        "    return clean_output(raw_text, prompt)\n",
        "\n",
        "def clean_output(text, prompt):\n",
        "    if text.startswith(prompt):\n",
        "        text = text[len(prompt):].strip()\n",
        "    last_punc = max(text.rfind(p) for p in [\".\", \"!\", \"?\"])\n",
        "    return text[:last_punc + 1].strip() if last_punc != -1 else text.strip()\n",
        "\n",
        "def get_inputs():\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"📬 AI EMAIL GENERATOR\".center(60))\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    sender = input(\"✍️  Enter your name (Sender): \").strip()\n",
        "    recipient = input(\"👤 Who is the recipient? (e.g., Manager, Friend): \").strip()\n",
        "    purpose = input(\"📝 What is the purpose of the email? (e.g., job application, invite): \").strip()\n",
        "\n",
        "    print(\"\\n🧾 Enter bullet points you want included (type 'done' to finish):\")\n",
        "    points = []\n",
        "    while True:\n",
        "        item = input(\"➤ \").strip()\n",
        "        if item.lower() == \"done\":\n",
        "            break\n",
        "        if item:\n",
        "            points.append(item)\n",
        "\n",
        "    print(\"\\n🎯 Choose the tone of your email:\\n1. Formal\\n2. Informal\")\n",
        "    tone = \"formal\" if input(\"Select option (1 or 2): \").strip() == \"1\" else \"informal\"\n",
        "\n",
        "    return purpose, recipient, points, tone, sender\n",
        "\n",
        "def main():\n",
        "    purpose, recipient, points, tone, sender = get_inputs()\n",
        "    prompt = build_prompt(purpose, recipient, points, tone, sender)\n",
        "    print(\"\\n✨ Generating your email. Please wait...\")\n",
        "    result = generate_text(prompt)\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"📨 Generated Email:\\n\")\n",
        "    print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyurBzkT5ued",
        "outputId": "c02a769f-7983-4e1f-8c25-422cb23ae879"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔄 Loading the model. Please wait...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model loaded successfully!\n",
            "\n",
            "============================================================\n",
            "                    📬 AI EMAIL GENERATOR                    \n",
            "============================================================\n",
            "✍️  Enter your name (Sender): yash\n",
            "👤 Who is the recipient? (e.g., Manager, Friend): ashmit\n",
            "📝 What is the purpose of the email? (e.g., job application, invite): invitation to my birthday party\n",
            "\n",
            "🧾 Enter bullet points you want included (type 'done' to finish):\n",
            "➤ date- 8th july\n",
            "➤ time -7pm\n",
            "➤ venue- landmark hotels\n",
            "➤ dress code- suit\n",
            "➤ done\n",
            "\n",
            "🎯 Choose the tone of your email:\n",
            "1. Formal\n",
            "2. Informal\n",
            "Select option (1 or 2): 2\n",
            "\n",
            "✨ Generating your email. Please wait...\n",
            "\n",
            "============================================================\n",
            "📨 Generated Email:\n",
            "\n",
            "Invitation to your birthday party\n",
            "\n",
            "Dear Ashmit,\n",
            "\n",
            "I hope this email finds you in good health and spirits. I wanted to invite you to my birthday party, which is on 8th July at Landmark Hotels. As you are the only guest who can attend this party, I believe it will be a great opportunity for us to celebrate together.\n",
            "\n",
            "The venue for the party is Landmark Hotels. The dress code is suit, and I suggest you wear a suit to make a grand entrance. The party will begin at 7pm and will continue until midnight.\n",
            "\n",
            "I am excited about the party and hope you will join us. Please let me know if you have any other queries or if there is anything else you need me to do.\n",
            "\n",
            "Best regards,\n",
            "\n",
            "Yash\n",
            "\n",
            "I hope this email helps you understand the invitation. If you have any questions, please do not hesitate to contact me.\n"
          ]
        }
      ]
    }
  ]
}